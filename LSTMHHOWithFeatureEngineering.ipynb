{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMS/kZKGSCOP4tNt2LzK0xs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidzangiabadi/LSTM-HHO-IoMT-Colab/blob/colab/LSTMHHOWithFeatureEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2_KiPFa6No06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e491544-4523-42da-eda2-cc689e5564a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
            "CLEAN LSTM (No Data Leakage)\n",
            "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
            "\n",
            "======================================================================\n",
            "üéÆ GPU: Tesla T4\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üîß FINAL PREPROCESSING (No Leakage)\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Loaded: 38,582 rows\n",
            "\n",
            "======================================================================\n",
            "üîí REMOVING POTENTIALLY LEAKY FEATURES\n",
            "======================================================================\n",
            "‚ö†Ô∏è  Dropping 10 suspicious columns:\n",
            "   - Unnamed: 0.1\n",
            "   - Unnamed: 0\n",
            "   - fin_flag_number\n",
            "   - syn_flag_number\n",
            "   - rst_flag_number\n",
            "   - psh_flag_number\n",
            "   - ack_flag_number\n",
            "   - ece_flag_number\n",
            "   - cwr_flag_number\n",
            "   - Number\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üìä DATA ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Shape: (38582, 32)\n",
            "\n",
            "üìä Class Distribution:\n",
            "label\n",
            "-1.0    19291\n",
            " 1.0    19291\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentage:\n",
            "label\n",
            "-1.0    50.0\n",
            " 1.0    50.0\n",
            "Name: count, dtype: float64\n",
            "\n",
            "‚úÖ Missing values: 0\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Features before selection: 31\n",
            "======================================================================\n",
            "üéØ FEATURE SELECTION (Robust)\n",
            "======================================================================\n",
            "\n",
            "üìä Top 20 features:\n",
            "Header_Length    0.669052\n",
            "Tot size         0.662800\n",
            "AVG              0.662725\n",
            "Tot sum          0.661843\n",
            "Max              0.653617\n",
            "IAT              0.652756\n",
            "Rate             0.651521\n",
            "ack_count        0.634988\n",
            "Time_To_Live     0.598907\n",
            "Std              0.590112\n",
            "Variance         0.588769\n",
            "TCP              0.469084\n",
            "UDP              0.460790\n",
            "HTTPS            0.376082\n",
            "Min              0.281569\n",
            "ARP              0.130318\n",
            "IPv              0.129732\n",
            "LLC              0.126120\n",
            "DHCP             0.086173\n",
            "DNS              0.057001\n",
            "dtype: float64\n",
            "\n",
            "‚úÖ Selected 20 features (MI > 0.01)\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Features after selection: 20\n",
            "‚úÖ Classes: 2\n",
            "‚úÖ Samples: 38,582\n",
            "\n",
            "‚úÖ Train: 24,685, Val: 6,172, Test: 7,715\n",
            "\n",
            "‚öñÔ∏è  Class Weights: tensor([1.0002, 0.9998])\n",
            "======================================================================\n",
            "\n",
            "üî® Building Model...\n",
            "\n",
            "üöÄ Training...\n",
            "Epoch 1/20 | Train: 99.57% | Val: 100.00% | F1: 100.00%\n",
            "Epoch 2/20 | Train: 100.00% | Val: 100.00% | F1: 100.00%\n",
            "Epoch 3/20 | Train: 99.99% | Val: 100.00% | F1: 100.00%\n",
            "Epoch 4/20 | Train: 100.00% | Val: 100.00% | F1: 100.00%\n",
            "Epoch 5/20 | Train: 99.99% | Val: 100.00% | F1: 100.00%\n",
            "Epoch 6/20 | Train: 100.00% | Val: 99.98% | F1: 99.98%\n",
            "\n",
            "‚ö†Ô∏è  Early Stopping\n",
            "\n",
            "‚è±Ô∏è  Training Time: 14.55s\n",
            "\n",
            "======================================================================\n",
            "üìä FINAL TEST RESULTS\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'numpy.float64' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3424525622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3424525622.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n‚è±Ô∏è  Training Time: {elapsed:.2f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üèÜ FINAL:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3424525622.py\u001b[0m in \u001b[0;36mevaluate_final\u001b[0;34m(model, test_loader, device, label_encoder)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìä FINAL TEST RESULTS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0mlongest_last_line_heading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m         \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongest_last_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0mhead_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:>{width}s} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" {:>9}\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0mlongest_last_line_heading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m         \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongest_last_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0mhead_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:>{width}s} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" {:>9}\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import os\n",
        "import warnings\n",
        "import gc\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def setup_device():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        device = torch.device('cuda:0')\n",
        "        print(\"=\"*70)\n",
        "        print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "        return device\n",
        "    return torch.device('cpu')\n",
        "\n",
        "# ==================== ÿ≠ÿ∞ŸÅ Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß€å Leaky ====================\n",
        "def remove_leaky_features(df: pd.DataFrame):\n",
        "    \"\"\"ÿ≠ÿ∞ŸÅ Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß€å ⁄©Ÿá ÿ®ÿßÿπÿ´ Data Leakage ŸÖ€å‚Äåÿ¥ŸàŸÜÿØ\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîí REMOVING POTENTIALLY LEAKY FEATURES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß€å ŸÖÿ¥⁄©Ÿà⁄© ÿ®Ÿá Leakage\n",
        "    leaky_patterns = ['Unnamed', 'index', 'id', 'ID', 'Index', 'Number']\n",
        "\n",
        "    original_cols = df.columns.tolist()\n",
        "    cols_to_drop = []\n",
        "\n",
        "    for col in original_cols:\n",
        "        for pattern in leaky_patterns:\n",
        "            if pattern.lower() in col.lower():\n",
        "                cols_to_drop.append(col)\n",
        "                break\n",
        "\n",
        "    if cols_to_drop:\n",
        "        print(f\"‚ö†Ô∏è  Dropping {len(cols_to_drop)} suspicious columns:\")\n",
        "        for col in cols_to_drop:\n",
        "            print(f\"   - {col}\")\n",
        "        df = df.drop(columns=cols_to_drop, errors='ignore')\n",
        "    else:\n",
        "        print(\"‚úÖ No suspicious columns found\")\n",
        "\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "    return df\n",
        "\n",
        "# ==================== ÿ™ÿ≠ŸÑ€åŸÑ ⁄©€åŸÅ€åÿ™ ÿØÿßÿØŸá (ÿ≥ÿßÿØŸá‚Äåÿ¥ÿØŸá) ====================\n",
        "def analyze_data_simple(df: pd.DataFrame):\n",
        "    \"\"\"ÿ™ÿ≠ŸÑ€åŸÑ ÿ≥ÿßÿØŸá ÿØÿßÿØŸá\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"üìä DATA ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\n‚úÖ Shape: {df.shape}\")\n",
        "\n",
        "    # ÿ™Ÿàÿ≤€åÿπ ⁄©ŸÑÿßÿ≥\n",
        "    print(\"\\nüìä Class Distribution:\")\n",
        "    label_counts = df['label'].value_counts()\n",
        "    print(label_counts)\n",
        "    print(\"\\nPercentage:\")\n",
        "    print((label_counts / len(df) * 100).round(2))\n",
        "\n",
        "    # Missing values\n",
        "    missing = df.isnull().sum().sum()\n",
        "    print(f\"\\n‚úÖ Missing values: {missing}\")\n",
        "\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ==================== Feature Selection ====================\n",
        "def select_features_robust(X: pd.DataFrame, y: np.ndarray, k: int = 20):\n",
        "    \"\"\"ÿßŸÜÿ™ÿÆÿßÿ® Ÿà€å⁄ò⁄Ø€å ÿ®ÿß MI (ÿ®ÿØŸàŸÜ ÿ≥ÿ™ŸàŸÜ‚ÄåŸáÿß€å ID-like)\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"üéØ FEATURE SELECTION (Robust)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
        "    mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "    print(f\"\\nüìä Top {min(k, len(mi_scores))} features:\")\n",
        "    print(mi_scores.head(k))\n",
        "\n",
        "    # ÿßŸÜÿ™ÿÆÿßÿ® features ÿ®ÿß MI ŸÖÿπŸÜÿßÿØÿßÿ±\n",
        "    threshold = 0.01  # ÿ≠ÿØÿßŸÇŸÑ MI\n",
        "    selected = mi_scores[mi_scores > threshold].head(k).index.tolist()\n",
        "\n",
        "    print(f\"\\n‚úÖ Selected {len(selected)} features (MI > {threshold})\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return selected\n",
        "\n",
        "# ==================== Dataset ====================\n",
        "class CleanCIC2024Dataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray, num_classes: int, augment: bool = False):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.astype(np.int64), dtype=torch.long)\n",
        "        self.num_classes = num_classes\n",
        "        self.augment = augment\n",
        "\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        self.class_weights = torch.zeros(num_classes)\n",
        "        for cls, count in zip(unique, counts):\n",
        "            self.class_weights[cls] = len(y) / (num_classes * count)\n",
        "\n",
        "        self.sample_weights = torch.tensor([self.class_weights[label].item() for label in y])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.X[idx], self.y[idx]\n",
        "\n",
        "        if self.augment and torch.rand(1).item() > 0.5:\n",
        "            noise = torch.randn_like(x) * 0.01\n",
        "            x = x + noise\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def get_sample_weights(self):\n",
        "        return self.sample_weights\n",
        "\n",
        "# ==================== Ÿæ€åÿ¥‚ÄåŸæÿ±ÿØÿßÿ≤ÿ¥ ŸÜŸáÿß€å€å ====================\n",
        "def preprocess_final(\n",
        "    file_path: str,\n",
        "    seq_len: int = 10,\n",
        "    batch_size: int = 128,\n",
        "    top_k_features: int = 20\n",
        "):\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîß FINAL PREPROCESSING (No Leakage)\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"‚úÖ Loaded: {len(df):,} rows\\n\")\n",
        "\n",
        "    # ÿ≠ÿ∞ŸÅ Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß€å Leaky\n",
        "    df = remove_leaky_features(df)\n",
        "\n",
        "    # ÿ™ÿ≠ŸÑ€åŸÑ\n",
        "    analyze_data_simple(df)\n",
        "\n",
        "    df_clean = df.dropna()\n",
        "\n",
        "    # ÿ¨ÿØÿßÿ≥ÿßÿ≤€å X Ÿà y\n",
        "    X = df_clean.select_dtypes(include=[np.number]).drop(columns=['label'], errors='ignore')\n",
        "    y = df_clean['label'].values\n",
        "\n",
        "    print(f\"‚úÖ Features before selection: {X.shape[1]}\")\n",
        "\n",
        "    # Encoding\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "    num_classes = len(np.unique(y_encoded))\n",
        "\n",
        "    # Feature Selection (ÿ®ÿØŸàŸÜ ID columns)\n",
        "    selected_features = select_features_robust(X, y_encoded, k=top_k_features)\n",
        "    X = X[selected_features]\n",
        "\n",
        "    print(f\"‚úÖ Features after selection: {X.shape[1]}\")\n",
        "    print(f\"‚úÖ Classes: {num_classes}\")\n",
        "    print(f\"‚úÖ Samples: {len(X):,}\\n\")\n",
        "\n",
        "    # Scaling\n",
        "    scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # Sequences\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X_scaled) - seq_len):\n",
        "        X_seq.append(X_scaled[i:i+seq_len])\n",
        "        y_seq.append(y_encoded[i+seq_len])\n",
        "\n",
        "    X_seq = np.array(X_seq, dtype=np.float32)\n",
        "    y_seq = np.array(y_seq, dtype=np.int64)\n",
        "\n",
        "    # Stratified Split\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Train: {len(X_train):,}, Val: {len(X_val):,}, Test: {len(X_test):,}\\n\")\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset = CleanCIC2024Dataset(X_train, y_train, num_classes, augment=True)\n",
        "    val_dataset = CleanCIC2024Dataset(X_val, y_val, num_classes, augment=False)\n",
        "    test_dataset = CleanCIC2024Dataset(X_test, y_test, num_classes, augment=False)\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=train_dataset.get_sample_weights(),\n",
        "        num_samples=len(train_dataset),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    input_size = X_seq.shape[2]\n",
        "    class_weights = train_dataset.class_weights\n",
        "\n",
        "    print(f\"‚öñÔ∏è  Class Weights: {class_weights}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, input_size, num_classes, label_encoder, class_weights\n",
        "\n",
        "# ==================== ŸÖÿØŸÑ ====================\n",
        "class SimpleLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, num_classes: int, dropout: float = 0.3):\n",
        "        super(SimpleLSTMModel, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_size * 2)\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.bn(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# ==================== ÿ¢ŸÖŸàÿ≤ÿ¥ ====================\n",
        "def train_final(model, train_loader, val_loader, device, class_weights, epochs=20, lr=0.001):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_correct, train_total = 0, 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device, non_blocking=True)\n",
        "            y_batch = y_batch.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_total += y_batch.size(0)\n",
        "            train_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "        train_acc = 100.0 * train_correct / train_total\n",
        "\n",
        "        # Val\n",
        "        model.eval()\n",
        "        val_correct, val_total = 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch = X_batch.to(device, non_blocking=True)\n",
        "                y_batch = y_batch.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += y_batch.size(0)\n",
        "                val_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "        val_f1 = f1_score(all_labels, all_preds, average='weighted') * 100\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train: {train_acc:.2f}% | Val: {val_acc:.2f}% | F1: {val_f1:.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model_clean.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= 5:\n",
        "                print(f\"\\n‚ö†Ô∏è  Early Stopping\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model_clean.pth'))\n",
        "    return best_val_acc\n",
        "\n",
        "# ==================== ÿßÿ±ÿ≤€åÿßÿ®€å ====================\n",
        "def evaluate_final(model, test_loader, device, label_encoder):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(device, non_blocking=True)\n",
        "            y_batch = y_batch.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä FINAL TEST RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    accuracy = 100.0 * (np.array(all_preds) == np.array(all_labels)).sum() / len(all_labels)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100\n",
        "\n",
        "    print(f\"\\n‚úÖ Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"‚úÖ Test F1-Score: {f1:.2f}%\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return accuracy, f1\n",
        "\n",
        "# ==================== Main ====================\n",
        "def main():\n",
        "    print(\"\\n\" + \"üéØ\"*35)\n",
        "    print(\"CLEAN LSTM (No Data Leakage)\")\n",
        "    print(\"üéØ\"*35 + \"\\n\")\n",
        "\n",
        "    device = setup_device()\n",
        "\n",
        "    train_loader, val_loader, test_loader, input_size, num_classes, label_encoder, class_weights = \\\n",
        "        preprocess_final(\n",
        "            file_path='final_sample_dataset.csv',\n",
        "            seq_len=10,\n",
        "            batch_size=128,\n",
        "            top_k_features=20\n",
        "        )\n",
        "\n",
        "    print(\"üî® Building Model...\")\n",
        "    model = SimpleLSTMModel(\n",
        "        input_size=input_size,\n",
        "        hidden_size=64,\n",
        "        num_layers=2,\n",
        "        num_classes=num_classes,\n",
        "        dropout=0.3\n",
        "    )\n",
        "\n",
        "    print(\"\\nüöÄ Training...\")\n",
        "    start = time.time()\n",
        "\n",
        "    val_acc = train_final(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        device=device,\n",
        "        class_weights=class_weights,\n",
        "        epochs=20,\n",
        "        lr=0.001\n",
        "    )\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"\\n‚è±Ô∏è  Training Time: {elapsed:.2f}s\")\n",
        "\n",
        "    test_acc, test_f1 = evaluate_final(model, test_loader, device, label_encoder)\n",
        "\n",
        "    print(f\"üèÜ FINAL:\")\n",
        "    print(f\"   Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"   Test Acc: {test_acc:.2f}%\")\n",
        "    print(f\"   Test F1: {test_f1:.2f}%\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ÿ®ÿ±ÿ±ÿ≥€å ÿ™Ÿàÿ≤€åÿπ\n",
        "df = pd.read_csv('final_sample_dataset.csv')\n",
        "print(\"üìä ÿ™Ÿàÿ≤€åÿπ ⁄©ŸÑÿßÿ≥‚ÄåŸáÿß:\")\n",
        "print(df['label'].value_counts())\n",
        "print(\"\\nÿØÿ±ÿµÿØ:\")\n",
        "print(df['label'].value_counts(normalize=True) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi0QSPVxWWko",
        "outputId": "d5013487-c676-4b1b-aad9-6c0a9b34862f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä ÿ™Ÿàÿ≤€åÿπ ⁄©ŸÑÿßÿ≥‚ÄåŸáÿß:\n",
            "label\n",
            "-1.0    19291\n",
            " 1.0    19291\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ÿØÿ±ÿµÿØ:\n",
            "label\n",
            "-1.0    50.0\n",
            " 1.0    50.0\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iDp8iPtkWWWI"
      }
    }
  ]
}